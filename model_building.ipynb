{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsDnrhqJr6OT",
        "outputId": "ccf7af5e-b9b3-4b2b-ff4f-57b28eb3d5eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading images from folder: Cataract_median_equalized\n",
            "Processing folder: /content/drive/My Drive/Ocular Dataset/Cataract_median_equalized\n",
            "Reading images from folder: diabetic_retinopathy_median_equalized\n",
            "Processing folder: /content/drive/My Drive/Ocular Dataset/diabetic_retinopathy_median_equalized\n",
            "Reading images from folder: normal_median_equalized\n",
            "Processing folder: /content/drive/My Drive/Ocular Dataset/normal_median_equalized\n",
            "Reading images from folder: glaucoma_median_equalized\n",
            "Processing folder: /content/drive/My Drive/Ocular Dataset/glaucoma_median_equalized\n",
            "Folder 'Cataract_median_equalized' contains 207 images.\n",
            "Folder 'diabetic_retinopathy_median_equalized' contains 219 images.\n",
            "Folder 'normal_median_equalized' contains 214 images.\n",
            "Folder 'glaucoma_median_equalized' contains 201 images.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the base directory where your folders are stored\n",
        "base_dir = '/content/drive/My Drive/Ocular Dataset'\n",
        "\n",
        "# List of folder names to read images from\n",
        "folders = [\n",
        "    \"Cataract_median_equalized\",\n",
        "    \"diabetic_retinopathy_median_equalized\",\n",
        "    \"normal_median_equalized\",\n",
        "    \"glaucoma_median_equalized\"\n",
        "]\n",
        "\n",
        "# Dictionary to store images from each folder\n",
        "images_by_folder = {}\n",
        "\n",
        "# Function to read images from a specified folder and store in the dictionary\n",
        "def read_images(folder_name):\n",
        "    \"\"\"\n",
        "    Reads images from a folder in Google Drive and stores them in a dictionary.\n",
        "    \"\"\"\n",
        "    # Path to the specific folder\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder not found: {folder_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing folder: {folder_path}\")\n",
        "\n",
        "    # Get all image file names in the folder\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # List to store images from this folder\n",
        "    images = []\n",
        "\n",
        "    # Read and store each image in the list\n",
        "    for image_file in image_files:\n",
        "        # Read the image in color\n",
        "        img_path = os.path.join(folder_path, image_file)\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if image is not None:\n",
        "            images.append(image)\n",
        "        else:\n",
        "            print(f\"Could not read the image: {image_file}\")\n",
        "\n",
        "    # Store the list of images in the dictionary with the folder name as the key\n",
        "    images_by_folder[folder_name] = images\n",
        "\n",
        "# Loop through each folder and read the images\n",
        "for folder in folders:\n",
        "    print(f\"Reading images from folder: {folder}\")\n",
        "    read_images(folder)\n",
        "\n",
        "# Summary of images loaded\n",
        "for folder_name, images in images_by_folder.items():\n",
        "    print(f\"Folder '{folder_name}' contains {len(images)} images.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3R0EHdpsCer",
        "outputId": "b91b82d9-de08-4404-d776-aca48ab11488"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (11.0.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "\n",
        "# Base directory where subfolders are located\n",
        "base_dir = '/content/drive/My Drive/Ocular Dataset'\n",
        "\n",
        "\n",
        "# List of specific subfolders to process\n",
        "specific_subfolders = [ \"Cataract_median_equalized\",\n",
        "    \"diabetic_retinopathy_median_equalized\",\n",
        "    \"normal_median_equalized\",\n",
        "    \"glaucoma_median_equalized\"]\n",
        "\n",
        "# Filter subfolders to process only specific ones\n",
        "subfolders = [f for f in specific_subfolders if os.path.isdir(os.path.join(base_dir, f))]\n",
        "\n",
        "# Dictionary to store features for specific subfolders\n",
        "features_by_folder = {}\n",
        "\n",
        "# Function to extract HOG features from a grayscale image\n",
        "def extract_hog_features(image):\n",
        "    # Extract HOG features and a visualization image\n",
        "    hog_features, hog_image = hog(image, visualize=True, pixels_per_cell=(16, 16), cells_per_block=(1, 1), feature_vector=True)\n",
        "\n",
        "    # Rescale the HOG image for visualization (optional)\n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "    return hog_features, hog_image_rescaled\n",
        "\n",
        "# Function to read images from a folder and extract HOG features\n",
        "def read_and_extract_features(folder_path):\n",
        "    # List all image files in the folder\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Store HOG features from this folder\n",
        "    hog_features_list = []\n",
        "\n",
        "    # Loop through each image and extract HOG features\n",
        "    for image_file in image_files:\n",
        "        img_path = os.path.join(folder_path, image_file)\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is not None:\n",
        "            # Extract HOG features\n",
        "            hog_features, _ = extract_hog_features(image)\n",
        "\n",
        "            # Append the HOG features to the list\n",
        "            hog_features_list.append(hog_features)\n",
        "\n",
        "    return hog_features_list\n",
        "\n",
        "# Loop through each subfolder to extract HOG features\n",
        "for subfolder in subfolders:\n",
        "    print(f\"Processing subfolder: {subfolder}\")\n",
        "    folder_path = os.path.join(base_dir, subfolder)\n",
        "\n",
        "    # Extract features from the current subfolder\n",
        "    features = read_and_extract_features(folder_path)\n",
        "\n",
        "    # Store the extracted features in the dictionary\n",
        "    features_by_folder[subfolder] = features\n",
        "\n",
        "    # Provide feedback on the number of images processed\n",
        "    print(f\"Extracted HOG features from {len(features)} images in '{subfolder}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOVHu7qRt0J9",
        "outputId": "5a1e1f4d-5b30-4bfa-d1d3-413efd433bb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subfolder: Cataract_median_equalized\n",
            "Extracted HOG features from 207 images in 'Cataract_median_equalized'\n",
            "Processing subfolder: diabetic_retinopathy_median_equalized\n",
            "Extracted HOG features from 219 images in 'diabetic_retinopathy_median_equalized'\n",
            "Processing subfolder: normal_median_equalized\n",
            "Extracted HOG features from 214 images in 'normal_median_equalized'\n",
            "Processing subfolder: glaucoma_median_equalized\n",
            "Extracted HOG features from 201 images in 'glaucoma_median_equalized'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure\n",
        "\n",
        "# Base directory where subfolders are located\n",
        "base_dir =  '/content/drive/My Drive/Ocular Dataset'\n",
        "\n",
        "# List of specific subfolders to process\n",
        "specific_subfolders = [ \"Cataract_median_equalized\",\n",
        "    \"diabetic_retinopathy_median_equalized\",\n",
        "    \"normal_median_equalized\",\n",
        "    \"glaucoma_median_equalized\"]\n",
        "\n",
        "# Filter subfolders to process only specific ones\n",
        "subfolders = [f for f in specific_subfolders if os.path.isdir(os.path.join(base_dir, f))]\n",
        "\n",
        "# Dictionary to store features for specific subfolders\n",
        "features_by_folder = {}\n",
        "\n",
        "# Function to extract HOG features from a grayscale image\n",
        "def extract_hog_features(image):\n",
        "    # Extract HOG features and a visualization image\n",
        "    hog_features, hog_image = hog(image, visualize=True, pixels_per_cell=(16, 16), cells_per_block=(1, 1), feature_vector=True)\n",
        "\n",
        "    # Rescale the HOG image for visualization (optional)\n",
        "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
        "\n",
        "    return hog_features, hog_image_rescaled\n",
        "\n",
        "# Function to read images from a folder and extract HOG features\n",
        "def read_and_extract_features(folder_path):\n",
        "    # List all image files in the folder\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # Store HOG features from this folder\n",
        "    hog_features_list = []\n",
        "\n",
        "    # Loop through each image and extract HOG features\n",
        "    for image_file in image_files:\n",
        "        img_path = os.path.join(folder_path, image_file)\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is not None:\n",
        "            # Extract HOG features\n",
        "            hog_features, _ = extract_hog_features(image)\n",
        "\n",
        "            # Append the HOG features to the list\n",
        "            hog_features_list.append(hog_features)\n",
        "\n",
        "    return hog_features_list\n",
        "\n",
        "# Loop through each subfolder to extract HOG features\n",
        "for subfolder in subfolders:\n",
        "    print(f\"Processing subfolder: {subfolder}\")\n",
        "    folder_path = os.path.join(base_dir, subfolder)\n",
        "\n",
        "    # Extract features from the current subfolder\n",
        "    features = read_and_extract_features(folder_path)\n",
        "\n",
        "    # Store the extracted features in the dictionary\n",
        "    features_by_folder[subfolder] = features\n",
        "\n",
        "    # Provide feedback on the number of images processed\n",
        "    print(f\"Extracted HOG features from {len(features)} images in '{subfolder}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Uyzy1wst7fZ",
        "outputId": "52375685-af36-49b2-c6a9-47f93b192424"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing subfolder: Cataract_median_equalized\n",
            "Extracted HOG features from 207 images in 'Cataract_median_equalized'\n",
            "Processing subfolder: diabetic_retinopathy_median_equalized\n",
            "Extracted HOG features from 219 images in 'diabetic_retinopathy_median_equalized'\n",
            "Processing subfolder: normal_median_equalized\n",
            "Extracted HOG features from 214 images in 'normal_median_equalized'\n",
            "Processing subfolder: glaucoma_median_equalized\n",
            "Extracted HOG features from 201 images in 'glaucoma_median_equalized'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Base directory where subfolders are located\n",
        "base_dir = '/content/drive/My Drive/Ocular Dataset'\n",
        "\n",
        "# List of specific subfolders to process\n",
        "specific_subfolders = [ \"Cataract_median_equalized\",\n",
        "    \"diabetic_retinopathy_median_equalized\",\n",
        "    \"normal_median_equalized\",\n",
        "    \"glaucoma_median_equalized\"]\n",
        "# Dictionary to store segmented images for each subfolder\n",
        "segmented_by_folder = {}\n",
        "\n",
        "# Function to apply Otsu's Thresholding for segmentation\n",
        "def otsu_threshold_segmentation(image):\n",
        "    # Apply Gaussian Blur to reduce noise\n",
        "    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    # Apply Otsu's Thresholding to segment the image\n",
        "    _, segmented_image = cv2.threshold(\n",
        "        blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU\n",
        "    )\n",
        "\n",
        "    return segmented_image\n",
        "\n",
        "# Function to read images from a folder and apply segmentation\n",
        "def read_and_segment_images(folder_path):\n",
        "    # List all image files in the folder\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # List to store segmented images\n",
        "    segmented_images = []\n",
        "\n",
        "    # Loop through each image and apply segmentation\n",
        "    for image_file in image_files:\n",
        "        img_path = os.path.join(folder_path, image_file)\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is not None:\n",
        "            # Apply Otsu's Thresholding for segmentation\n",
        "            segmented_image = otsu_threshold_segmentation(image)\n",
        "\n",
        "            # Store the segmented image in the list\n",
        "            segmented_images.append({\n",
        "                'original_image': image,\n",
        "                'segmented_image': segmented_image,\n",
        "            })\n",
        "\n",
        "    return segmented_images\n",
        "\n",
        "# Loop through each specified subfolder to extract and store segmented images\n",
        "for subfolder in specific_subfolders:\n",
        "    print(f\"Processing segmentation for subfolder: {subfolder}\")\n",
        "    folder_path = os.path.join(base_dir, subfolder)\n",
        "\n",
        "    # Apply segmentation and store in memory\n",
        "    segmented_images = read_and_segment_images(folder_path)\n",
        "\n",
        "    # Store the segmented images in a dictionary\n",
        "    segmented_by_folder[subfolder] = segmented_images\n",
        "\n",
        "    # Provide feedback on the number of segmented images\n",
        "    print(f\"Segmented {len(segmented_images)} images in '{subfolder}'\")\n",
        "\n",
        "# Now `segmented_by_folder` contains all segmented images in memory, which can be used for further processing.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wflT3zQoupC2",
        "outputId": "a27bc454-6062-43cc-d597-18f3024ac605"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing segmentation for subfolder: Cataract_median_equalized\n",
            "Segmented 207 images in 'Cataract_median_equalized'\n",
            "Processing segmentation for subfolder: diabetic_retinopathy_median_equalized\n",
            "Segmented 219 images in 'diabetic_retinopathy_median_equalized'\n",
            "Processing segmentation for subfolder: normal_median_equalized\n",
            "Segmented 214 images in 'normal_median_equalized'\n",
            "Processing segmentation for subfolder: glaucoma_median_equalized\n",
            "Segmented 201 images in 'glaucoma_median_equalized'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU3PLlAWxTO7",
        "outputId": "996d9829-24eb-4ceb-bcae-060ccd5349b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil  # This will help to copy files and folders\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Base directory for the dataset\n",
        "base_dir = '/content/drive/My Drive/Ocular Dataset'\n",
        "\n",
        "\n",
        "# List of specific subfolders to process\n",
        "target_subfolders = [ \"Cataract_median_equalized\",\n",
        "    \"diabetic_retinopathy_median_equalized\",\n",
        "    \"normal_median_equalized\",\n",
        "    \"glaucoma_median_equalized\"]\n",
        "\n",
        "# Create a new temporary base directory\n",
        "temp_base_dir = '/content/drive/My Drive/Ocular Dataset/temp'\n",
        "\n",
        "# Ensure the temporary base directory is empty and exists\n",
        "if os.path.exists(temp_base_dir):\n",
        "    shutil.rmtree(temp_base_dir)  # Clear if exists\n",
        "os.makedirs(temp_base_dir, exist_ok=True)  # Create a fresh folder\n",
        "\n",
        "# Copy the specified subfolders to the temporary base directory\n",
        "for subfolder in target_subfolders:\n",
        "    src_path = os.path.join(base_dir, subfolder)\n",
        "    dest_path = os.path.join(temp_base_dir, subfolder)\n",
        "\n",
        "    # Copy the entire subfolder if it exists\n",
        "    if os.path.isdir(src_path):\n",
        "        shutil.copytree(src_path, dest_path)\n",
        "\n",
        "# Prepare the data generator for data augmentation\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Training dataset from specified subfolders\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Validation dataset from specified subfolders\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(128, 128),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define a simple CNN model\n",
        "cnn_model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')  # Output layer for categorical classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = cnn_model.evaluate(val_generator)\n",
        "print(f\"Validation accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S05TBeEYxnzT",
        "outputId": "7e31e078-9594-4694-cebf-1f6dbc3a76b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 675 images belonging to 4 classes.\n",
            "Found 166 images belonging to 4 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 1s/step - accuracy: 0.3124 - loss: 1.4269 - val_accuracy: 0.3675 - val_loss: 1.2476\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.4778 - loss: 1.0952 - val_accuracy: 0.5301 - val_loss: 1.1112\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.5620 - loss: 0.9294 - val_accuracy: 0.6386 - val_loss: 0.9262\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.6680 - loss: 0.7732 - val_accuracy: 0.5602 - val_loss: 0.9490\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.6869 - loss: 0.7343 - val_accuracy: 0.5964 - val_loss: 0.9106\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.6769 - loss: 0.7114 - val_accuracy: 0.6566 - val_loss: 0.7894\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.6952 - loss: 0.6874 - val_accuracy: 0.6386 - val_loss: 0.8136\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.7183 - loss: 0.6026 - val_accuracy: 0.6566 - val_loss: 0.8839\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.7702 - loss: 0.5214 - val_accuracy: 0.6807 - val_loss: 0.8042\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.8241 - loss: 0.4268 - val_accuracy: 0.6747 - val_loss: 0.7656\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 290ms/step - accuracy: 0.6702 - loss: 0.8280\n",
            "Validation accuracy: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Temporary base directory for the dataset\n",
        "temp_base_dir = '/content/drive/My Drive/Ocular Dataset/temp'\n",
        "# Prepare the data generator for data augmentation\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Training dataset from specified subfolders\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(227, 227),  # AlexNet uses 227x227 images\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Validation dataset from specified subfolders\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(227, 227),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define an AlexNet architecture\n",
        "alexnet_model = models.Sequential([\n",
        "    layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(227, 227, 3)),  # Conv1\n",
        "    layers.MaxPooling2D((3, 3), strides=(2, 2)),  # Pool1\n",
        "    layers.Conv2D(256, (5, 5), padding='same', activation='relu'),  # Conv2\n",
        "    layers.MaxPooling2D((3, 3), strides=(2, 2)),  # Pool2\n",
        "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),  # Conv3\n",
        "    layers.Conv2D(384, (3, 3), padding='same', activation='relu'),  # Conv4\n",
        "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),  # Conv5\n",
        "    layers.MaxPooling2D((3, 3), strides=(2, 2)),  # Pool3\n",
        "    layers.Flatten(),  # Flattening for fully connected layers\n",
        "    layers.Dense(4096, activation='relu'),  # FC6\n",
        "    layers.Dropout(0.5),  # Dropout for regularization\n",
        "    layers.Dense(4096, activation='relu'),  # FC7\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')  # Output layer for categorical classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "alexnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "alexnet_model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = alexnet_model.evaluate(val_generator)\n",
        "print(f\"Validation accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AftuuCiMzAY7",
        "outputId": "d1910887-f403-4f99-9ea9-2c4e36e2f060"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 675 images belonging to 4 classes.\n",
            "Found 166 images belonging to 4 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6s/step - accuracy: 0.2536 - loss: 2.5649 - val_accuracy: 0.2470 - val_loss: 1.3867\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - accuracy: 0.2492 - loss: 1.3555 - val_accuracy: 0.2530 - val_loss: 1.4051\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 6s/step - accuracy: 0.3413 - loss: 1.2863 - val_accuracy: 0.2590 - val_loss: 1.3945\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6s/step - accuracy: 0.3123 - loss: 1.2831 - val_accuracy: 0.2651 - val_loss: 1.3914\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 6s/step - accuracy: 0.3396 - loss: 1.2637 - val_accuracy: 0.2771 - val_loss: 1.3835\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6s/step - accuracy: 0.2827 - loss: 1.3161 - val_accuracy: 0.2771 - val_loss: 1.3610\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 6s/step - accuracy: 0.3380 - loss: 1.2657 - val_accuracy: 0.2771 - val_loss: 1.3841\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 6s/step - accuracy: 0.3499 - loss: 1.2967 - val_accuracy: 0.2831 - val_loss: 1.3679\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 6s/step - accuracy: 0.3729 - loss: 1.2701 - val_accuracy: 0.2771 - val_loss: 1.3150\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 6s/step - accuracy: 0.3946 - loss: 1.2315 - val_accuracy: 0.4518 - val_loss: 1.1937\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.4661 - loss: 1.1731\n",
            "Validation accuracy: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Temporary base directory for the dataset\n",
        "temp_base_dir = '/content/drive/My Drive/Ocular Dataset/temp'\n",
        "\n",
        "# Data generator with data augmentation and validation split\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Training dataset from specified subfolders\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(224, 224),  # VGG-like networks use 224x224 images\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Validation dataset from specified subfolders\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define a simplified VGG-like architecture\n",
        "simple_vgg = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')  # Output for categorical classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "simple_vgg.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "simple_vgg.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = simple_vgg.evaluate(val_generator)\n",
        "print(f\"Validation accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itBwPPZl0svK",
        "outputId": "dc5f130a-94f5-4186-a3f7-026a71c55859"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 675 images belonging to 4 classes.\n",
            "Found 166 images belonging to 4 classes.\n",
            "Epoch 1/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 8s/step - accuracy: 0.2880 - loss: 1.4955 - val_accuracy: 0.4096 - val_loss: 1.3427\n",
            "Epoch 2/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 8s/step - accuracy: 0.4613 - loss: 1.2141 - val_accuracy: 0.4458 - val_loss: 1.1608\n",
            "Epoch 3/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 8s/step - accuracy: 0.5059 - loss: 1.0732 - val_accuracy: 0.4759 - val_loss: 1.0919\n",
            "Epoch 4/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 8s/step - accuracy: 0.5793 - loss: 0.8907 - val_accuracy: 0.5181 - val_loss: 1.0046\n",
            "Epoch 5/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 8s/step - accuracy: 0.5645 - loss: 0.8243 - val_accuracy: 0.5181 - val_loss: 0.9513\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.5043 - loss: 0.9895\n",
            "Validation accuracy: 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from skimage.feature import hog\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "\n",
        "# Dictionary to store features by folder\n",
        "features_by_folder = {}\n",
        "\n",
        "# Function to read images and extract HOG features\n",
        "def read_and_extract_features(folder_path, image_size=(128, 128)):\n",
        "    features = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg', '.jpeg', '.png')):  # Add other extensions if needed\n",
        "                image_path = os.path.join(root, file)\n",
        "                image = imread(image_path, as_gray=True)  # Read image in grayscale\n",
        "                image_resized = resize(image, image_size)  # Resize image\n",
        "                hog_features = hog(image_resized, pixels_per_cell=(8, 8),\n",
        "                                   cells_per_block=(2, 2), block_norm='L2-Hys')  # Extract HOG\n",
        "                features.append(hog_features)\n",
        "    return features\n",
        "\n",
        "# Base directory for the dataset\n",
        "base_dir = '/content/drive/My Drive/Ocular Dataset'\n",
        "\n",
        "# Main folders with their subfolders\n",
        "main_folders = {\n",
        "    \"Cataract\": [\"Cataract_low\", \"Cataract_medium\", \"Cataract_high\"],\n",
        "    \"Diabetic Retinopathy\": [\"Diabetic Retinopathy_low\", \"Diabetic Retinopathy_medium\", \"Diabetic Retinopathy_high\"],\n",
        "    \"Normal\": [],  # No subfolders for Normal\n",
        "    \"Glaucoma\": [\"Glaucoma_low\", \"Glaucoma_medium\", \"Glaucoma_high\"],\n",
        "}\n",
        "\n",
        "# Loop through main folders and their subfolders\n",
        "for main_folder, subfolders in main_folders.items():\n",
        "    if subfolders:\n",
        "        for subfolder in subfolders:\n",
        "            folder_path = os.path.join(base_dir, main_folder, subfolder)\n",
        "            if os.path.exists(folder_path):\n",
        "                # Extract features from the current subfolder\n",
        "                features = read_and_extract_features(folder_path)\n",
        "                features_by_folder[f\"{main_folder}/{subfolder}\"] = features\n",
        "                print(f\"Extracted HOG features from {len(features)} images in '{main_folder}/{subfolder}'\")\n",
        "    else:\n",
        "        folder_path = os.path.join(base_dir, main_folder)\n",
        "        if os.path.exists(folder_path):\n",
        "            # Extract features from the main folder if there are no subfolders\n",
        "            features = read_and_extract_features(folder_path)\n",
        "            features_by_folder[main_folder] = features\n",
        "            print(f\"Extracted HOG features from {len(features)} images in '{main_folder}'\")\n",
        "\n",
        "# Now 'features_by_folder' contains HOG features from all specified folders and subfolders\n"
      ],
      "metadata": {
        "id": "KazYhTVqBaFA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from skimage.feature import hog\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "\n",
        "# Dictionary to store features by folder\n",
        "features_by_folder = {}\n",
        "\n",
        "# Function to read images and extract HOG features\n",
        "def read_and_extract_features(folder_path, image_size=(128, 128)):\n",
        "    features = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(('.jpg', '.jpeg', '.png')):  # Add other extensions if needed\n",
        "                image_path = os.path.join(root, file)\n",
        "                image = imread(image_path, as_gray=True)  # Read image in grayscale\n",
        "                image_resized = resize(image, image_size)  # Resize image\n",
        "                hog_features = hog(image_resized, pixels_per_cell=(8, 8),\n",
        "                                   cells_per_block=(2, 2), block_norm='L2-Hys')  # Extract HOG\n",
        "                features.append(hog_features)\n",
        "    return features\n",
        "\n",
        "# Base directory for the dataset\n",
        "base_dir = '/content/drive/My Drive/Ocular Dataset'\n",
        "\n",
        "# Main folders with their subfolders\n",
        "main_folders = {\n",
        "    \"Cataract\": [\"Cataract_low\", \"Cataract_medium\", \"Cataract_high\"],\n",
        "    \"Diabetic Retinopathy\": [\"Diabetic Retinopathy_low\", \"Diabetic Retinopathy_medium\", \"Diabetic Retinopathy_high\"],\n",
        "    \"Normal\": [],  # No subfolders for Normal\n",
        "    \"Glaucoma\": [\"Glaucoma_low\", \"Glaucoma_medium\", \"Glaucoma_high\"],\n",
        "}\n",
        "\n",
        "# Loop through main folders and their subfolders\n",
        "for main_folder, subfolders in main_folders.items():\n",
        "    if subfolders:\n",
        "        for subfolder in subfolders:\n",
        "            folder_path = os.path.join(base_dir, main_folder, subfolder)\n",
        "            if os.path.exists(folder_path):\n",
        "                # Extract features from the current subfolder\n",
        "                features = read_and_extract_features(folder_path)\n",
        "                features_by_folder[f\"{main_folder}/{subfolder}\"] = features\n",
        "                print(f\"Extracted HOG features from {len(features)} images in '{main_folder}/{subfolder}'\")\n",
        "    else:\n",
        "        folder_path = os.path.join(base_dir, main_folder)\n",
        "        if os.path.exists(folder_path):\n",
        "            # Extract features from the main folder if there are no subfolders\n",
        "            features = read_and_extract_features(folder_path)\n",
        "            features_by_folder[main_folder] = features\n",
        "            print(f\"Extracted HOG features from {len(features)} images in '{main_folder}'\")\n",
        "\n",
        "# Now 'features_by_folder' contains HOG features from all specified folders and subfolders\n"
      ],
      "metadata": {
        "id": "2rKB7IKQGeJM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Temporary base directory for the dataset\n",
        "temp_base_dir = '/content/drive/My Drive/Ocular Dataset/temp'\n",
        "\n",
        "# Data generator with data augmentation and validation split\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Training dataset from specified subfolders\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(224, 224),  # EfficientNetB0 input size (224x224)\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Validation dataset from specified subfolders\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    temp_base_dir,\n",
        "    target_size=(224, 224),  # EfficientNetB0 input size (224x224)\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Load the pre-trained EfficientNetB0 model without the top layers (for transfer learning)\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add custom top layers for our specific problem\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)  # Global Average Pooling\n",
        "x = layers.Dense(1024, activation='relu')(x)  # Fully connected layer\n",
        "x = layers.Dropout(0.5)(x)  # Dropout for regularization\n",
        "x = layers.Dense(512, activation='relu')(x)  # Another fully connected layer\n",
        "x = layers.Dropout(0.5)(x)  # Dropout for regularization\n",
        "output = layers.Dense(train_generator.num_classes, activation='softmax')(x)  # Output layer for classification\n",
        "\n",
        "# Create the final model\n",
        "model = models.Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze the EfficientNet layers (they won't be trained)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FONPhOR0KbBf",
        "outputId": "95649bbb-6207-46d2-eda4-8ae08d1d911e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 675 images belonging to 4 classes.\n",
            "Found 166 images belonging to 4 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - accuracy: 0.2418 - loss: 1.4142 - val_accuracy: 0.2530 - val_loss: 1.3938\n",
            "Epoch 2/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - accuracy: 0.2221 - loss: 1.4173 - val_accuracy: 0.2530 - val_loss: 1.3913\n",
            "Epoch 3/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - accuracy: 0.2907 - loss: 1.3988 - val_accuracy: 0.2590 - val_loss: 1.3983\n",
            "Epoch 4/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step - accuracy: 0.2668 - loss: 1.4215 - val_accuracy: 0.2410 - val_loss: 1.3886\n",
            "Epoch 5/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.2186 - loss: 1.4027 - val_accuracy: 0.2590 - val_loss: 1.3874\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.2828 - loss: 1.3850\n",
            "Validation accuracy: 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of EfficientNetB0\n",
        "for layer in base_model.layers[-10:]:  # Unfreeze the last 10 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile the model to reflect the changes\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train again\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pFFvHSbLjS8",
        "outputId": "3a7c9c1a-8bda-43b6-8594-efbce9b7f962"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - accuracy: 0.2480 - loss: 2.2602 - val_accuracy: 0.2590 - val_loss: 1.3878\n",
            "Epoch 2/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 3s/step - accuracy: 0.2741 - loss: 1.9993 - val_accuracy: 0.2590 - val_loss: 1.3882\n",
            "Epoch 3/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2564 - loss: 1.9358 - val_accuracy: 0.2590 - val_loss: 1.3884\n",
            "Epoch 4/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step - accuracy: 0.2842 - loss: 1.7228 - val_accuracy: 0.2590 - val_loss: 1.3885\n",
            "Epoch 5/5\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.2531 - loss: 1.8306 - val_accuracy: 0.2590 - val_loss: 1.3885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c2d8ada1210>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a learning rate scheduler\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-3 * 0.95**epoch, verbose=1\n",
        ")\n",
        "\n",
        "# Train with learning rate scheduler\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[lr_scheduler]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv0AAEGJObvS",
        "outputId": "f7b84b5d-2f1e-4151-df37-3c5484be725b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step - accuracy: 0.2615 - loss: 1.5265 - val_accuracy: 0.2590 - val_loss: 1.3903 - learning_rate: 0.0010\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.00095.\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.2663 - loss: 1.4326 - val_accuracy: 0.2590 - val_loss: 1.3890 - learning_rate: 9.5000e-04\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0009025.\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.2592 - loss: 1.4165 - val_accuracy: 0.2590 - val_loss: 1.3870 - learning_rate: 9.0250e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.000857375.\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step - accuracy: 0.2477 - loss: 1.4141 - val_accuracy: 0.2590 - val_loss: 1.3864 - learning_rate: 8.5737e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.0008145062499999999.\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.2478 - loss: 1.4120 - val_accuracy: 0.2470 - val_loss: 1.3863 - learning_rate: 8.1451e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0007737809374999998.\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2647 - loss: 1.3908 - val_accuracy: 0.2470 - val_loss: 1.3874 - learning_rate: 7.7378e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0007350918906249999.\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - accuracy: 0.2433 - loss: 1.4026 - val_accuracy: 0.2410 - val_loss: 1.3882 - learning_rate: 7.3509e-04\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0006983372960937497.\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3s/step - accuracy: 0.2442 - loss: 1.3951 - val_accuracy: 0.2530 - val_loss: 1.3863 - learning_rate: 6.9834e-04\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0006634204312890623.\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - accuracy: 0.3062 - loss: 1.3814 - val_accuracy: 0.2470 - val_loss: 1.3875 - learning_rate: 6.6342e-04\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0006302494097246091.\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 4s/step - accuracy: 0.2124 - loss: 1.4058 - val_accuracy: 0.2470 - val_loss: 1.3865 - learning_rate: 6.3025e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c2d6ac0fd60>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,            # Random zoom\n",
        "    width_shift_range=0.2,     # Random horizontal shift\n",
        "    height_shift_range=0.2,    # Random vertical shift\n",
        "    shear_range=0.2,           # Random shear transformation\n",
        "    fill_mode='nearest'        # Fill in missing pixels after transformations\n",
        ")\n"
      ],
      "metadata": {
        "id": "B6mMj4yvQOQw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add more capacity to the model\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(512, activation='relu')(x)  # Increase neurons\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(train_generator.num_classes, activation='softmax')(x)\n"
      ],
      "metadata": {
        "id": "tNi3DR52TNFg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use RMSprop optimizer\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J278LyiSTPE7",
        "outputId": "fe2c85a5-d1f0-442a-c0af-610850ba451e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 3s/step - accuracy: 0.2269 - loss: 1.4155 - val_accuracy: 0.2530 - val_loss: 1.3937\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.2722 - loss: 1.3896 - val_accuracy: 0.2590 - val_loss: 1.3908\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 4s/step - accuracy: 0.2171 - loss: 1.4069 - val_accuracy: 0.2590 - val_loss: 1.4155\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.2977 - loss: 1.4037 - val_accuracy: 0.2530 - val_loss: 1.4028\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.2720 - loss: 1.4099 - val_accuracy: 0.2590 - val_loss: 1.4003\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step - accuracy: 0.2431 - loss: 1.4037 - val_accuracy: 0.2590 - val_loss: 1.5272\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.2453 - loss: 1.4087 - val_accuracy: 0.2530 - val_loss: 1.3908\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.2482 - loss: 1.3909 - val_accuracy: 0.2530 - val_loss: 1.4135\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.2634 - loss: 1.4033 - val_accuracy: 0.2410 - val_loss: 1.3995\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 4s/step - accuracy: 0.2606 - loss: 1.3971 - val_accuracy: 0.2410 - val_loss: 1.3934\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c2d856eec80>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for more epochs (e.g., 20 or 50)\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "x = base_model.output\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)  # Add Batch Normalization\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train with early stopping\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znugqghGTSUn",
        "outputId": "149ee807-639e-4fb4-ca19-41ca75a103f4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.2553 - loss: 1.3930 - val_accuracy: 0.2410 - val_loss: 1.3945\n",
            "Epoch 2/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2522 - loss: 1.3985 - val_accuracy: 0.2530 - val_loss: 1.4570\n",
            "Epoch 3/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2176 - loss: 1.4077 - val_accuracy: 0.2530 - val_loss: 1.6367\n",
            "Epoch 4/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.2430 - loss: 1.3918 - val_accuracy: 0.2590 - val_loss: 1.4348\n",
            "Epoch 5/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2207 - loss: 1.3992 - val_accuracy: 0.2590 - val_loss: 1.4077\n",
            "Epoch 6/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2480 - loss: 1.3951 - val_accuracy: 0.2530 - val_loss: 1.4311\n",
            "Epoch 7/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.2509 - loss: 1.4025 - val_accuracy: 0.2530 - val_loss: 1.3863\n",
            "Epoch 8/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.2369 - loss: 1.3945 - val_accuracy: 0.2590 - val_loss: 1.4045\n",
            "Epoch 9/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - accuracy: 0.2152 - loss: 1.3936 - val_accuracy: 0.2590 - val_loss: 1.4941\n",
            "Epoch 10/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 3s/step - accuracy: 0.2475 - loss: 1.4003 - val_accuracy: 0.2590 - val_loss: 1.4032\n",
            "Epoch 11/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2046 - loss: 1.3993 - val_accuracy: 0.2590 - val_loss: 1.4073\n",
            "Epoch 12/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.2531 - loss: 1.3946 - val_accuracy: 0.2590 - val_loss: 1.3865\n",
            "Epoch 13/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.2581 - loss: 1.3938 - val_accuracy: 0.2470 - val_loss: 1.3866\n",
            "Epoch 14/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - accuracy: 0.2009 - loss: 1.3933 - val_accuracy: 0.2590 - val_loss: 1.3865\n",
            "Epoch 15/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.2187 - loss: 1.3933 - val_accuracy: 0.2590 - val_loss: 1.3886\n",
            "Epoch 16/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step - accuracy: 0.2413 - loss: 1.3942 - val_accuracy: 0.2590 - val_loss: 1.3861\n",
            "Epoch 17/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.2801 - loss: 1.3858 - val_accuracy: 0.2530 - val_loss: 1.3860\n",
            "Epoch 18/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.2647 - loss: 1.3891 - val_accuracy: 0.2590 - val_loss: 1.3864\n",
            "Epoch 19/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.2657 - loss: 1.3842 - val_accuracy: 0.2590 - val_loss: 1.3880\n",
            "Epoch 20/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2760 - loss: 1.3926 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 1/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 3s/step - accuracy: 0.2336 - loss: 1.3913 - val_accuracy: 0.2590 - val_loss: 1.3876\n",
            "Epoch 2/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.2587 - loss: 1.3890 - val_accuracy: 0.2590 - val_loss: 1.3875\n",
            "Epoch 3/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 3s/step - accuracy: 0.2522 - loss: 1.3890 - val_accuracy: 0.2590 - val_loss: 1.3881\n",
            "Epoch 4/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.2594 - loss: 1.3899 - val_accuracy: 0.2590 - val_loss: 1.3903\n",
            "Epoch 5/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.2386 - loss: 1.3891 - val_accuracy: 0.2590 - val_loss: 1.3930\n",
            "Epoch 6/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 3s/step - accuracy: 0.2680 - loss: 1.3867 - val_accuracy: 0.2590 - val_loss: 1.3889\n",
            "Epoch 7/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 3s/step - accuracy: 0.2443 - loss: 1.3876 - val_accuracy: 0.2530 - val_loss: 1.3860\n",
            "Epoch 8/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.2624 - loss: 1.3875 - val_accuracy: 0.2530 - val_loss: 1.3861\n",
            "Epoch 9/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 3s/step - accuracy: 0.2857 - loss: 1.3866 - val_accuracy: 0.2530 - val_loss: 1.3862\n",
            "Epoch 10/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.2570 - loss: 1.3887 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 11/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3s/step - accuracy: 0.2300 - loss: 1.3919 - val_accuracy: 0.2530 - val_loss: 1.3860\n",
            "Epoch 12/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2513 - loss: 1.3862 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 13/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.2485 - loss: 1.3861 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 14/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.2512 - loss: 1.3889 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 15/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 4s/step - accuracy: 0.2610 - loss: 1.3864 - val_accuracy: 0.2530 - val_loss: 1.3860\n",
            "Epoch 16/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 4s/step - accuracy: 0.2713 - loss: 1.3829 - val_accuracy: 0.2590 - val_loss: 1.3861\n",
            "Epoch 17/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - accuracy: 0.2369 - loss: 1.3891 - val_accuracy: 0.2590 - val_loss: 1.3859\n",
            "Epoch 18/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.2547 - loss: 1.3876 - val_accuracy: 0.2590 - val_loss: 1.3859\n",
            "Epoch 19/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.2563 - loss: 1.3855 - val_accuracy: 0.2590 - val_loss: 1.3859\n",
            "Epoch 20/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.2383 - loss: 1.3874 - val_accuracy: 0.2590 - val_loss: 1.3861\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c2d6b26eb60>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Early stopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train with early stopping\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaonzsQiTeol",
        "outputId": "02f4f52c-2a38-4cbe-d564-22fa7d9b8cba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - accuracy: 0.2143 - loss: 1.3919 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 2/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 4s/step - accuracy: 0.2440 - loss: 1.3895 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 3/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.2189 - loss: 1.3887 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 4/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step - accuracy: 0.2445 - loss: 1.3911 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 5/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 3s/step - accuracy: 0.2577 - loss: 1.3866 - val_accuracy: 0.2590 - val_loss: 1.3860\n",
            "Epoch 6/20\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - accuracy: 0.2691 - loss: 1.3897 - val_accuracy: 0.2590 - val_loss: 1.3861\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c2d840f7fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example data: Replace these values with your model results\n",
        "models = ['EfficientNet', 'ResNet', 'DenseNet', 'VGG']\n",
        "train_accuracy = [0.82, 0.78, 0.84, 0.80]  # Replace with actual train accuracy\n",
        "val_accuracy = [0.79, 0.75, 0.81, 0.77]    # Replace with actual validation accuracy\n",
        "test_accuracy = [0.78, 0.74, 0.80, 0.76]   # Replace with actual test accuracy\n",
        "\n",
        "# Plot bar chart\n",
        "x = np.arange(len(models))  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bar1 = ax.bar(x - width, train_accuracy, width, label='Train Accuracy', color='blue')\n",
        "bar2 = ax.bar(x, val_accuracy, width, label='Validation Accuracy', color='green')\n",
        "bar3 = ax.bar(x + width, test_accuracy, width, label='Test Accuracy', color='orange')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Models', fontsize=12)\n",
        "ax.set_ylabel('Accuracy', fontsize=12)\n",
        "ax.set_title('Model Accuracy Comparison', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models, fontsize=10)\n",
        "ax.legend()\n",
        "\n",
        "# Add accuracy values above bars\n",
        "def add_values(bars):\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "add_values(bar1)\n",
        "add_values(bar2)\n",
        "add_values(bar3)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Msu_ZEQso4_f",
        "outputId": "47f6cc21-61c4-4eb2-c9f2-79e8e4667649"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEOEFBUMsqL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}